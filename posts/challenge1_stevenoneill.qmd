---
title: "Challenge 1"
author: "Steve O'Neill"
desription: "Reading in data and creating a post"
date: "08/15/2022"
format:
  html:
    toc: true
    code-fold: true
    code-copy: true
    code-tools: true
categories:
  - challenge_1
  - tidyverse
---

```{r}
#| label: setup
#| warning: false
#| message: false

library(tidyverse)

knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

## Challenge Overview

Today's challenge is to

1)  read in a dataset, and

2)  describe the dataset using both words and any supporting information (e.g., tables, etc)

## Read in the Data

Read in one (or more) of the following data sets, using the correct R package and command.

-   railroad_2012_clean_county.csv ⭐
-   birds.csv ⭐⭐
-   FAOstat\*.csv ⭐⭐
-   wild_bird_data.xlsx ⭐⭐⭐
-   **StateCounty2012.xlsx** ⭐⭐⭐⭐

Find the `_data` folder, located inside the `posts` folder. Then you can read in the data, using either one of the `readr` standard tidy read commands, or a specialized package such as `readxl`.

```{r}
library(readxl)
```

```{r}
#Importing the dataset, and removing the first two rows of unhelpful data:
df1 <- read_xls("_data/StateCounty2012.xls", skip = 2)

#I really tried, but I couldn't get the relative paths to work :( 
# getwd() returned "C:/Users/stevenoneill/Downloads/dacss601_august2022"
#df1 <- read_xls("./posts/_data/StateCounty2012.xls", skip = 2)
#df1 <- read_xls("_data/StateCounty2012.xls", skip = 2)

#I used my Downloads folder because I would rather not exempt my Documents folder from OneDrive. Willing to die on this hill.

```

```{r}
#First, removing unused columns
df2 <- df1 %>% select(STATE,COUNTY,TOTAL)
```

```{r}
#Second, removing unhelpful pre-calculated totals:
df3=df2[grepl("^[a-zA-Z][a-zA-Z]$",df2$STATE),]
```

*Add any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation.*

## Describe the data

*Using a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).*

This data describes railroad employment in U.S. states and territories. In this instance, the original 'cases' are the *counties* and the original 'variables' are their parent states and the total number of persons employed in the railroad industry in those counties.

You may notice the data contains the uncommon state codes "AE" and "AP", as well as the recurring "APO" county name. These represent [military addresses](https://pe.usps.com/text/pub28/28c2_010.htm):

| State Code | Location                            |
|------------|-------------------------------------|
| AE         | Europe, Middle East, Africa, Canada |
| AP         | Asia Pacific                        |
| AA         | Americas (excluding Canada)         |

'APO' refers to "Army Post Office".

I have calculated some basic statistics:

```{r}
#Group by state, but first, add the largest county to the dataframe
by_state <- df3 %>% group_by(STATE) %>% 
    mutate(largest.county.name = COUNTY[which.max(TOTAL)]) %>% 
      mutate(smallest.county.name = COUNTY[which.min(TOTAL)])

#Group by state, then summarize the total of all county employees, per-state:
by_state <- by_state %>% summarise(
  total.state.employees = sum(TOTAL),
  median.county.employees = median(TOTAL),
  smallest.county = min(TOTAL),
  smallest.county.name = first(smallest.county.name),
  largest.county = max(TOTAL),
  largest.county.name = first(largest.county.name),
  standard.dev = sd(TOTAL)
)

by_state
```

A few things stand out:

-   Texas has the largest amount of railroad employees combined, at 19,839.
-   However, Illinois has the largest single county of railroad employees in Cook County, at 8207. That's almost double the next-largest in Tarrant, TX.
-   Illinois also possesses one of the smallest counties by the same metric - Hardin County, with only one employee. It has the highest standard deviation among in-state counties.

Looking forward to the next steps in analysis.